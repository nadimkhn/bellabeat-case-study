---
title: "Fitness Tracker Data Analysis"
author: "Nadim"
date: "2024-12-11"
output: 
  html_document:
    theme: 'cerulean'
    toc: yes
    toc_depth: 3
    toc_float: yes
---

## Introduction

As a part of the Google Data Analytics Certification, I am analyzing user data collected from their personal fitness devices, to gain insights into usage habit. This analysis aims to provide actionable recommendations to guide [Bellabeat's](https://bellabeat.com/) marketing strategy. Bellabeat is a high-tech manufacturer of health-focused smart products.

I will follow the six-step data analysis process: ask, prepare, process, analyze, share and act as introduced in the certification course. This document demonstrates key skills acquired through the program and present use cases relevant to stakeholder's at Bellabeat.

## 1. Ask
*Deliverable: A summary of the business task and key stakeholders.*

**Business Task:** The task at hand is to analyze the Fitbit Fitness Tracker Data to identify trends in smart device usage and apply these insights to enhance Bellabeat's marketing strategy. The key questions addressed in this analysis are:

* What are some trends in smart device usage?
* How could these trends apply to Bellabeat customers?
* How could these trends help influence Bellabeat marketing strategy?

**Key Stakeholders:**

* Urška Sršen: Bellabeat’s cofounder and Chief Creative Officer
* Sando Mur: Mathematician and Bellabeat’s cofounder;
* Bellabeat marketing analytics team

## 2. Prepare
*Deliverable: A description of all data sources used.*

**Data Source**

**[Fitbit Fitness Tracker Data]((https://www.kaggle.com/datasets/arashnic/fitbit/data)) (CC0: Public Domain, dataset made available through [Mobius](https://www.kaggle.com/arashnic))**

This dataset is openly available on [Kaggle](https://www.kaggle.com/datasets/arashnic/fitbit/data) which contains personal tracker data, including minute-level output for physical activity, heart rate, and sleep monitoring of 30 Fitbit users. This dataset was generated by participants of a distributed survey via Amazon Mechanical Turk between 03/12/2016 - 05/12/2016. Let's prepare and get familiar with the data for analysis.

**Inital Setup**

To prepare the environment for analysis, I will load the following R packages:
*`tidyverse`: For data manipulation and visualization.
*`skimr`: To provide a quick summary of the data.
*`janitor`:For data cleaning and ensuring consistency in column names.

```{r installing packages}

library(dplyr)
library(tidyverse)
library(skimr)
library(janitor)
library(scales)
library(corrplot)
options(scipen = 999)
```

**Data Description**

The data is stored in .csv format, split across 29 files under two folders, each representing different time intervals:

*Folder 1: 03/12/2014 - 04/11/2016
*Folder 2: 04/12/2016 - 05/12/2016

The files contain daily, hourly, and minute-level outputs on various measures, including:

*Steps
*Calories burned
*Intensities
*Heart rate
*Sleep monitoring

Since the goal is to identify high-level trends in usage, the focus will primarily be on daily and hourly datasets, as they provide sufficient granularity for this purpose.

**Importing the Datasets**

Let's start by importing the csv files into dataframes for exploration.

```{r import daily activity datasets}

# Importing 03/12/2016-04/11/16 datasets

daily_activity_03_12 <- read_csv("~/Desktop/PROJECTS/bellabeat-case-study/data/Fitabase Data 3.12.16-4.11.16/dailyActivity_merged.csv")

# Daily aggregated data for intensities, calories, steps and sleep is present between 03/12/2016-04/11/2016, so I will use the hourly and minute level outputs to compute daily totals.

hourly_calories_03_12 <- read_csv("~/Desktop/PROJECTS/bellabeat-case-study/data/Fitabase Data 3.12.16-4.11.16/hourlyCalories_merged.csv")
hourly_steps_03_12 <- read_csv("~/Desktop/PROJECTS/bellabeat-case-study/data/Fitabase Data 3.12.16-4.11.16/hourlySteps_merged.csv")
minute_sleep_03_12 <- read_csv("~/Desktop/PROJECTS/bellabeat-case-study/data/Fitabase Data 3.12.16-4.11.16/minuteSleep_merged.csv")
weight_log_03_12 <- read_csv("~/Desktop/PROJECTS/bellabeat-case-study/data/Fitabase Data 3.12.16-4.11.16/weightLogInfo_merged.csv")

# Importing 04/12/16-05/12/2016 datasets

daily_activity_04_12 <- read_csv("~/Desktop/PROJECTS/bellabeat-case-study/data/Fitabase Data 4.12.16-5.12.16/dailyActivity_merged.csv")

hourly_calories_04_12 <- read_csv("~/Desktop/PROJECTS/bellabeat-case-study/data/Fitabase Data 4.12.16-5.12.16/hourlyCalories_merged.csv")
hourly_steps_04_12 <- read_csv("~/Desktop/PROJECTS/bellabeat-case-study/data/Fitabase Data 4.12.16-5.12.16/hourlySteps_merged.csv")
minute_sleep_04_12 <- read_csv("~/Desktop/PROJECTS/bellabeat-case-study/data/Fitabase Data 4.12.16-5.12.16/minuteSleep_merged.csv")
weight_log_04_12 <- read_csv("~/Desktop/PROJECTS/bellabeat-case-study/data/Fitabase Data 4.12.16-5.12.16/weightLogInfo_merged.csv")
```
Let's explore and compare the datasets to see how its organized.

```{r dataframe preview}

head(daily_activity_03_12)
head(daily_activity_04_12)

head(hourly_calories_03_12)
head(hourly_calories_04_12)

head(minute_sleep_03_12)
head(minute_sleep_04_12)

head(hourly_steps_03_12)
head(hourly_steps_04_12)

head(weight_log_03_12)
head(weight_log_04_12)
```

**Key Observations**

Primary Key: The Id column is common across all dataframes, making it suitable as a primary key for merging or combining datasets when needed.

Date Formatting: The date columns are not in the correct data type, these need to be converted to Date or DateTime formats.

Time Granularity: The data is recorded at various time intervals: minute, hour, and day. To ensure consistency in analysis, the datasets need to be transformed into a uniform time granularity, such as daily summaries.

**Data Credibility**

Limited Sample Size: The dataset includes about 33 participants, making it difficult to generalize findings to a broader population.

Outdated Data: The data was collected in 2016 over a span of two months, limiting its relevance to current trends.

Lack of Demographic Information: The absence of demographic details prevents the assessment of sampling bias, further reducing the dataset's reliability.

## 3. Process
*Deliverable: Documentation of any cleaning or manipulation of data*

**Date Formatting: Converting the datatype of date columns into datetime datatype**

Let's start by converting the data formatting for the datasets and since we are never going to perform arithmetic operations on participant ID number, lets covert the column to string.

```{r datatype formatting}

daily_activity_03_12 <- daily_activity_03_12 %>% 
  mutate(ActivityDate = mdy(ActivityDate),
         Id = as.character(Id))

daily_activity_04_12 <- daily_activity_04_12 %>% 
  mutate(ActivityDate = mdy(ActivityDate),
         Id = as.character(Id))

hourly_calories_03_12 <- hourly_calories_03_12 %>% 
  mutate(ActivityHour = mdy_hms(ActivityHour),
         Id = as.character(Id))

hourly_calories_04_12 <- hourly_calories_04_12 %>% 
  mutate(ActivityHour = mdy_hms(ActivityHour),
         Id = as.character(Id))

hourly_steps_03_12 <- hourly_steps_03_12 %>% 
  mutate(ActivityHour = mdy_hms(ActivityHour),
         Id = as.character(Id))

hourly_steps_04_12 <- hourly_steps_04_12 %>% 
  mutate(ActivityHour = mdy_hms(ActivityHour),
         Id = as.character(Id))

minute_sleep_03_12 <- minute_sleep_03_12 %>% 
  mutate(date = mdy_hms(date),
         Id = as.character(Id))

minute_sleep_04_12 <- minute_sleep_04_12 %>% 
  mutate(date = mdy_hms(date),
         Id = as.character(Id))

weight_log_03_12 <- weight_log_03_12 %>% 
  mutate(Date = mdy_hms(Date),
         Id = as.character(Id))

weight_log_04_12 <- weight_log_04_12 %>% 
  mutate(Date = mdy_hms(Date),
         Id = as.character(Id))
```

**Data Cleaning: Removing duplicates and identifying outliers/missing data**

Before diving into analysis, it's crucial to clean the data by handling duplicates, missing values, and outliers. We'll start by looking at quick summary statistics to identify these issues.

```{r data summary}

# No. of unique participants in each datasets

n_distinct(daily_activity_03_12$Id) 
n_distinct(daily_activity_04_12$Id)

n_distinct(hourly_calories_03_12$Id)
n_distinct(hourly_calories_04_12$Id)

n_distinct(hourly_steps_03_12$Id)
n_distinct(hourly_steps_04_12$Id)

n_distinct(minute_sleep_03_12$Id)
n_distinct(minute_sleep_04_12$Id)

n_distinct(weight_log_03_12$Id)
n_distinct(weight_log_04_12$Id)

# Summary stats for each datasets

summary(daily_activity_03_12)
summary(daily_activity_04_12)

summary(hourly_calories_03_12)
summary(hourly_calories_04_12)

summary(hourly_steps_03_12)
summary(hourly_steps_04_12)

summary(minute_sleep_03_12)
summary(minute_sleep_04_12)

summary(weight_log_03_12)
summary(weight_log_04_12)

```

**Inconsistent number of unique participants**

We can see that the datasets for daily activity, calories and steps between the 03/12/2016-04/11/2016 interval has more number of participants than the 04/12/2016-05/12/2016 files. The file for sleep related data contains data for only 23 and 24 participants for the two time frames and weight related datasets consists data on 11 and 8 unqiue participants *indicating not every participants utilized these two features*. However, it can help us analyze daily device usage and its feature overtime so I will include the additional participants data in my analysis.

**Incomplete or irrelevant data**

If we look at the minimum and maximum values for datetime columns between the two time intervals, I found that the data for the 04/12/2016 date is reported twice in both the time interval files for all the datasets. If we also examine the datetime stamp I can confirm that the 03/12/2016-04/11/2016 datasets contains data only for the mornings of the 04/12/2016 whereas the later date interval files have data for the whole day.

Let's plot the number of daily observations from both the time intervals on a bar chart to further explore the data reported for 04/12/2016 date to deal with duplicate/incomplete data if any exists. 

```{r data cleaning & vertical merge}

ggplot(bind_rows("df 1" = hourly_calories_03_12, "df 2" = hourly_calories_04_12, .id = "groups"), aes(as_date(ActivityHour), fill = groups)) +
  geom_bar()

# Using distinct on Id and Activity Hour to drop any duplicates

hourly_calories <- bind_rows("df 1" = hourly_calories_03_12, "df 2" = hourly_calories_04_12, .id = "groups") %>%
  arrange(by = desc(ActivityHour), desc(groups)) %>% 
  distinct(Id, ActivityHour, .keep_all = TRUE)

# plotting the data again

ggplot(hourly_calories, aes(as_date(ActivityHour), fill = groups)) +
  geom_bar()
``` 
Let's repeat the merging and cleaning steps for other datasets.

```{r data cleaning & vertical merge 2}

hourly_steps <- bind_rows("df 1" = hourly_steps_03_12, "df 2" = hourly_steps_04_12, .id = "groups") %>%
  arrange(by = desc(ActivityHour), desc(groups)) %>% 
  distinct(Id, ActivityHour, .keep_all = TRUE)

ggplot(hourly_steps, aes(as_date(ActivityHour), fill = groups)) +
  geom_bar()

# sleep dataset

minute_sleep <- bind_rows("df 1" = minute_sleep_03_12, "df 2" = minute_sleep_04_12, .id = "groups") %>%
  arrange(by = desc(date), desc(groups)) %>% 
  distinct(Id, date, .keep_all = TRUE)

ggplot(minute_sleep, aes(as_date(date), fill = groups)) +
  geom_bar()

# activity dataset

daily_activity <- bind_rows("df 1" = daily_activity_03_12, "df 2" = daily_activity_04_12, .id = "groups") %>%
  arrange(by = desc(ActivityDate), desc(groups)) %>% 
  distinct(Id, ActivityDate, .keep_all = TRUE)

ggplot(daily_activity) +
  geom_bar(mapping = aes(ActivityDate, fill = groups))

# weight log

weight_log <- bind_rows("df 1" = weight_log_03_12, "df 2" = weight_log_04_12, .id = "groups") %>%
  arrange(by = desc(Date), desc(groups)) %>% 
  distinct(Id, Date, .keep_all = TRUE)

ggplot(weight_log) +
  geom_bar(mapping = aes(as_date(Date), fill = groups))

# Remove previous datasets

rm(daily_activity_03_12, daily_activity_04_12,
   hourly_calories_03_12, hourly_calories_04_12,
   hourly_steps_03_12, hourly_steps_04_12,
   minute_sleep_03_12, minute_sleep_04_12,
   weight_log_03_12, weight_log_04_12)
```

To ensure consistency across datasets, hourly and minute level datasets will be aggregated into daily summaries. This transformation consolidates data into uniform format, making it easier to compare and analyze trends.

```{r data transformation}

daily_calories <- hourly_calories %>% 
  mutate(ActivityDate = as_date(ActivityHour)) %>% 
  group_by(Id, ActivityDate) %>% 
  summarise(Calories = sum(Calories)) %>% 
  arrange(Id, ActivityDate)

daily_steps <- hourly_steps %>% 
  mutate(ActivityDate = as_date(ActivityHour)) %>% 
  group_by(Id, ActivityDate) %>% 
  summarise(StepTotal = sum(StepTotal)) %>% 
  arrange(Id, ActivityDate)

daily_sleep <- minute_sleep %>%
  mutate(date = as_date(date)) %>% 
  group_by(Id, date) %>% 
  summarise(TotalSleepRecords = n_distinct(logId),
            TotalMinutesAsleep = sum(value == 1),
            TotalTimeInBed = n(),
            .groups = "drop") %>% 
  arrange(Id, date)

# Remove hourly and minute level datasets

rm(hourly_calories, hourly_steps,
   minute_sleep)
```

Finally, let's the merge the datasets into a single single source of truth for further exploration and analysis.

```{r merging horizontally}

daily_activity <- daily_activity %>% 
  select(-Calories, -TotalSteps, -groups) %>% 
  full_join(daily_calories, join_by("Id" == "Id", "ActivityDate" == "ActivityDate")) %>% 
  full_join(daily_steps, join_by("Id" == "Id", "ActivityDate" == "ActivityDate")) %>%
  clean_names()

weight_log <- weight_log %>% 
  select(-groups)

rm(daily_calories, daily_steps)
```


Let's finish this step by performing some data validation techniques to ensure the final dataframe has no duplicates and ready for analysis and clean up the environment by dropping other dataframes.

```{r data validation}

# Check for duplicates

sum(duplicated(daily_activity))
sum(duplicated(daily_sleep))
sum(duplicated(weight_log))

# Clean column names

daily_activity <- clean_names(daily_activity)
daily_sleep <- clean_names(daily_sleep)
weight_log <- clean_names(weight_log)

```

## 4. Analyze
*Deliverable: A summary of your analysis*

Let's start with a summary statistics of all columns in our final datasets.

```{r summary stats}

skim_without_charts(daily_activity)
skim_without_charts(daily_sleep)
skim_without_charts(weight_log)

```

The study lasted over 62 days with 35 participants and as we can see that there's fair amount of null values for each variable, we can assume that the participants didn't use the device everyday. Let's extract insights on the daily device usage of the participants.

```{r device usage calculation}
 
# Calculating avg number of days participants used for tracking activities

# daily activity
daily_activity %>% 
  group_by(id) %>% 
  summarise(activity_records = n()) %>% 
  summary()

# daily sleep

daily_sleep %>% 
  group_by(id) %>% 
  summarise(activity_records = n()) %>% 
  summary()

# weight log

weight_log %>% 
  group_by(id) %>% 
  summarise(num_of_records = n()) %>% 
  summary()
  
```

On average participants used the device for 56 days out of the 62 for tracking their daily activities and 34 days while sleeping. Additionally, out of all the participants, ten of them never used the device to track sleeping habits and fewer than one third of users used wight log, and usage was irregular.

Let's compare the type of activities users spend their time in a day.

```{r activity type plot}

ggplot(daily_activity %>% 
         pivot_longer(cols = c(very_active_minutes, fairly_active_minutes, lightly_active_minutes, sedentary_minutes),
                      names_to = "activity_type",
                      values_to = "minutes") %>%
         drop_na() %>% 
         group_by(activity_type) %>% 
         summarise(avg_time_spent = mean(minutes)) %>%
         mutate(percentage = avg_time_spent / sum(avg_time_spent)*100),
       aes(x = percentage, y = reorder(activity_type, avg_time_spent))) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste(round(percentage, 2), "%", sep = " ")), hjust = -0.05, vjust = -0.25, size = 3) +
  scale_x_continuous(breaks = seq(0, 800, 100))

```
On average participants spend over 80% of their time being sedentary with minimal time in very active or fairly active zones.. 

```{r correlation plots}

merged_data <- daily_activity %>% 
         full_join(daily_sleep, join_by("id" == "id", "activity_date" == "date"))

numeric_df <- merged_data %>% 
  select(where(is.numeric))

cor_matrix <- cor(numeric_df, use = "pairwise.complete.obs")

corrplot(cor_matrix, order = "hclust", 
         type = "lower",
         tl.col = "black", 
         tl.srt = 10)

```
- Calories vs Steps/Distance: Strong positive correlation
    - Step total & total distance are primary predictors of caloric expenditure
- Sedentary Minutes vs Sleep: Strong negative correlation
    - More sedentary time associated with shorter sleep duration


Finally, let's look at weekly usage trends.

```{r weekdays trends}
ggplot(daily_activity %>% 
         mutate(day_of_week = wday(activity_date, label = TRUE, week_start = 1)) %>%
         drop_na() %>% 
         group_by(day_of_week) %>% 
         summarise(avg_cals = mean(calories)), aes(x = avg_cals, y = day_of_week)) +
  geom_bar(stat = "identity")

ggplot(daily_activity %>%
         mutate(day_of_week = wday(activity_date, label = TRUE, week_start = 1)) %>% 
         pivot_longer(cols = c(very_active_minutes, fairly_active_minutes, lightly_active_minutes, sedentary_minutes),
                      names_to = "activity_type",
                      values_to = "minutes") %>%
         drop_na() %>% 
         group_by(day_of_week, activity_type) %>% 
         summarise(avg_time_spent = mean(minutes)) %>%
         mutate(percentage = avg_time_spent / sum(avg_time_spent)*100), aes(x = percentage, y = day_of_week, fill = activity_type)) +
  geom_bar(stat = "identity")

ggplot(daily_sleep %>% 
         mutate(day_of_week = wday(date, label = TRUE, week_start = 1)) %>%
         drop_na() %>% 
         group_by(day_of_week) %>% 
         summarise(avg_asleep_mins = mean(total_minutes_asleep)), aes(x = avg_asleep_mins, y = day_of_week)) +
  geom_bar(stat = "identity")

```

- Higher calories burned & steps observed during weekdays vs weekends.
- Most active days: Wednesday & Thursday
- Sleep duration peaked on weekends, especially Sundays.

## 5. Share
*Deliverable: Your top high-level insights based on your analysis*

**Behavioural Treds**

- High engagement in step and distance tracking
- Sedentary lifestyle dominates daily time use
- Sleep and weight tracking are underutilized
- Weekdays drive higher activity; weekends favor rest

**Key Takeways**

1. Lean into what users already do well: Users already track steps & distance well, emphasize on step goals, milestone badges, or social leader boards to drive engagement.
2. Target sedentary users with nudge-based features to promote daily activity reminders, with narrative around how poor sleep correlates with high sedentary time.
3. Use weekday patterns for messaging timing: Time notifications, tips, and challenges to match user behavior (e.g., active midweek, rest on Sunday)
